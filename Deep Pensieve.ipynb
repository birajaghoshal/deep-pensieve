{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Pensieve™\n",
    "A Deep Residual MMD Variational Auto-Encoder with Group Normalization (GN), Efficient Sub-Pixel Convolution Super-Resolution (ESPCN), and Perceptual Similarity Loss (SSIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540551741.4923084-final.gif'></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540552110.470284-final.gif'></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-080.gif\"></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540552122.395882-final.gif'></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540551578.5925505-final.gif'></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1705.07202\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-1.png\">\n",
    "\n",
    "Variational auto-encoder (VAE) is a powerful unsupervised learning framework for image generation. One drawback of VAE is that it generates blurry images due to its Gaussianity assumption and thus L2 loss. To allow the generation of high quality images by VAE, we increase the capacity of decoder network by employing residual blocks and skip connections, which also enable efficient optimization. To overcome the limitation of L2 loss, we propose to generate images in a multi-stage manner from coarse to fine. In the simplest case, the proposed multi-stage VAE divides the decoder into two components in which the second component generates refined images based on the course images generated by the first component. Since the second component is independent of the VAE model, it can employ other loss functions beyond the L2 loss and different model architectures. The proposed framework can be easily generalized to contain more than two components. Experiment results on the MNIST and CelebA datasets demonstrate that the proposed multi-stage VAE can generate sharper images as compared to those from the original VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from libs import utils, gif\n",
    "from libs.group_norm import GroupNormalization\n",
    "from libs.variance_pooling import GlobalVariancePooling2D\n",
    "\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.layers import Input, Flatten, Reshape, Add, Multiply, Activation, Lambda\n",
    "from keras.layers import Dense, Conv2D, DepthwiseConv2D, SeparableConv2D, MaxPooling2D, UpSampling2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras_contrib.losses import DSSIMObjective\n",
    "from keras_contrib.layers.convolutional import SubPixelUpscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = 'roadtrip'\n",
    "\n",
    "SIZE = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "SCALE_FACTOR = 2\n",
    "\n",
    "FEATURES = SIZE*SIZE*CHANNELS\n",
    "FEATURES_2X = SCALE_FACTOR*SIZE*SCALE_FACTOR*SIZE*CHANNELS\n",
    "\n",
    "MODEL_NAME = DIRECTORY+'-'+str(SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images:\t184\n",
      "Loading images:\t184\n",
      "MODEL:  roadtrip-32\n",
      "IMGS:  (184, 32, 32, 3) (184, 64, 64, 3)\n",
      "FLAT:  (184, 3072) (184, 12288)\n",
      "SAMPLES:  (9, 3072) (9, 12288)\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "imgs, xs, ys  = utils.load_images(directory=\"imgs/\"+DIRECTORY,rx=SIZE,ry=SIZE)\n",
    "imgs_2x, xs_2x, ys_2x = utils.load_images(directory=\"imgs/\"+DIRECTORY,rx=SCALE_FACTOR*SIZE,ry=SCALE_FACTOR*SIZE)\n",
    "\n",
    "# normalize pixels\n",
    "IMGS = imgs/127.5 - 1\n",
    "FLAT = np.reshape(IMGS,(-1,FEATURES))\n",
    "\n",
    "IMGS_2X = imgs_2x/127.5 - 1\n",
    "FLAT_2X = np.reshape(IMGS_2X,(-1,FEATURES_2X)) \n",
    "\n",
    "SAMPLES =  np.random.permutation(FLAT)[:9]\n",
    "SAMPLES_2X =  np.random.permutation(FLAT_2X)[:9]\n",
    "\n",
    "TOTAL_BATCH = IMGS.shape[0]\n",
    "\n",
    "# print shapes\n",
    "print(\"MODEL: \",MODEL_NAME)\n",
    "print(\"IMGS: \",IMGS.shape,IMGS_2X.shape)\n",
    "print(\"FLAT: \",FLAT.shape,FLAT_2X.shape)\n",
    "print(\"SAMPLES: \",SAMPLES.shape,SAMPLES_2X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Deep Convolutional Networks for Large-Scale Image Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1409.1556\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/CascadingConvolutions.png\">\n",
    "\n",
    "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3×3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers.\n",
    "\n",
    "First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative.\n",
    "\n",
    "Second, we decrease the number of parameters: assuming that both the input and the output of a\n",
    "three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by (W) weights; at the same time, a single 7 × 7 conv. layer would require 81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/groupnorm.png\">\n",
    "\n",
    "\n",
    "The mainstream normalization technique for almost all convolutional neural networks today is Batch Normalization (BN), which has been widely adopted in the development of deep learning. Proposed by Google in 2015, BN can not only accelerate a model’s converging speed, but also alleviate problems such as Gradient Dispersion in the deep neural network, making it easier to train models.\n",
    "\n",
    "Dr. Wu and Dr. He however argue in their paper Group Normalization that normalizing with batch size has limitations, as BN cannot ensure the model accuracy rate when the batch size becomes smaller. As a result, researchers today are normalizing with large batches, which is very memory intensive, and are avoiding using limited memory to explore higher-capacity models.\n",
    "\n",
    "Dr. Wu and Dr. He believe their new GN technique is a simple but effective alternative to BN. Specifically, GN divides channels — also referred to as feature maps that look like 3D chunks of data — into groups and normalizes the features within each group. GN only exploits the layer dimensions, and its computation is independent of batch sizes.\n",
    "\n",
    "The paper reports that GN had a 10.6% lower error rate than its BN counterpart for ResNet-50 in ImageNet with a batch size of 2 samples; and matched BN performance while outperforming other normalization techniques with a regular batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    # set current layer\n",
    "    current_layer = Reshape((SIZE,SIZE,CHANNELS))(x)\n",
    "    \n",
    "    # convolution layers\n",
    "    for layer, n_filters in enumerate(FILTERS):\n",
    "\n",
    "        # stacked 3x3 convolutions with group normalization + activation\n",
    "        current_layer = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)(current_layer)\n",
    "        current_layer = GroupNormalization(groups=n_filters,axis=-1)(current_layer)\n",
    "        current_layer = Activation(ACTIVATION)(current_layer)\n",
    "\n",
    "        current_layer = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)(current_layer)\n",
    "        current_layer = GroupNormalization(groups=n_filters,axis=-1)(current_layer)\n",
    "        current_layer = Activation(ACTIVATION)(current_layer)\n",
    "         \n",
    "        # max pooling\n",
    "        current_layer = MaxPooling2D()(current_layer)\n",
    "    \n",
    "    # grab the last shape for reconstruction\n",
    "    shape = current_layer.get_shape().as_list()\n",
    "    \n",
    "    # flatten\n",
    "    flat = Flatten()(current_layer)\n",
    "    \n",
    "    # latent vector\n",
    "    z = Dense(LATENT_DIM,name='encoder')(flat)\n",
    "    \n",
    "    return z, (shape[1],shape[2],shape[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Mean Discrepancy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/kl_latent.gif\" ></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/mmd_latent.gif\" ></td>\n",
    "</tr></table>\n",
    "\n",
    "Maximum mean discrepancy (MMD, (Gretton et al. 2007)) is based on the idea that two distributions are identical if and only if all their moments are the same. Therefore, we can define a divergence by measuring how “different” the moments of two distributions p(z) and q(z) are. MMD can accomplish this efficiently via the kernel embedding trick:\n",
    "\n",
    "A kernel can be intuitively interpreted as a function that measures the “similarity” of two samples. It has a large value when two samples are similar, and small when they are different. For example, the Gaussian kernel considers points that are close in Euclidean space to be “similar”. A rough intuition of MMD, then, is that if two distributions are identical, then the average “similarity” between samples from each distribution, should be identical to the average “similarity” between mixed samples from both distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel(x, y):\n",
    "    x_size = tf.shape(x)[0]\n",
    "    y_size = tf.shape(y)[0]\n",
    "    dim = tf.shape(x)[1]\n",
    "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true,y_pred):\n",
    "    epsilon = tf.random_normal(tf.stack([BATCH_SIZE, LATENT_DIM]))\n",
    "    latent_loss = compute_mmd(epsilon, y_pred)\n",
    "    return latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution and Checkerboard Artifacts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "When we have neural networks generate images, we often have them build them up from low resolution, high-level descriptions. This allows the network to describe the rough image and then fill in the details.\n",
    "\n",
    "In order to do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. Roughly, deconvolution layers allow the model to use every point in the small image to “paint” a square in the larger one.\n",
    "\n",
    "Unfortunately, deconvolution can easily have “uneven overlap,” putting more of the metaphorical paint in some places than others. In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top). While the network could, in principle, carefully learn weights to avoid this  — as we’ll discuss in more detail later — in practice neural networks struggle to avoid it completely.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-2.png\">\n",
    "\n",
    "To avoid these artifacts, we’d like an alternative to regular deconvolution (“transposed convolution”). Unlike deconvolution, this approach to upsampling shouldn’t have artifacts as its default behavior. Ideally, it would go further, and be biased against such artifacts.\n",
    "\n",
    "One approach is to separate out upsampling to a higher resolution from convolution to compute features. For example, you might resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer. This seems like a natural approach, and roughly similar methods have worked well in image super-resolution.\n",
    "\n",
    "Our experience has been that nearest-neighbor resize followed by a convolution works very well, in a wide variety of contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(z,z_g,shape=None):\n",
    "    \n",
    "    # reverse the encoder\n",
    "    filters = FILTERS[::-1]\n",
    "\n",
    "    # inflate\n",
    "    inflated = shape[0]*shape[1]*shape[2]\n",
    "    inflate = Dense(inflated,name='generator')\n",
    "    current_layer = inflate(z) ; generator = inflate(z_g)\n",
    "    \n",
    "    # reshape\n",
    "    reshape = Reshape(shape)\n",
    "    current_layer = reshape(current_layer) ; generator = reshape(generator)\n",
    "    \n",
    "    # build layers\n",
    "    for layer, n_filters in enumerate(filters):\n",
    "        \n",
    "        # upsample\n",
    "        u = UpSampling2D()\n",
    "        current_layer = u(current_layer) ; generator = u(generator)\n",
    "\n",
    "        # stacked 3x3 convolutions with group normalization + activation\n",
    "        c1 = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "        b1 = GroupNormalization(groups=n_filters,axis=-1)\n",
    "        a1 = Activation(ACTIVATION)\n",
    "\n",
    "        current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "        current_layer = b1(current_layer) ; generator = b1(generator)\n",
    "        current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "\n",
    "        c2 = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "        b2 = GroupNormalization(groups=n_filters,axis=-1)\n",
    "        a2 = Activation(ACTIVATION)\n",
    "\n",
    "        current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "        current_layer = b2(current_layer) ; generator = b2(generator)\n",
    "        current_layer = a2(current_layer) ; generator = a2(generator)\n",
    "    \n",
    "    # output convolution + activation\n",
    "    conv = Conv2D(CHANNELS,1,padding='SAME',kernel_initializer=INITIALIZER_TANH)\n",
    "    activation = Activation('tanh',name='decoder_dssim')\n",
    "    \n",
    "    current_layer = conv(current_layer)       ; generator = conv(generator)\n",
    "    current_layer = activation(current_layer) ; generator = activation(generator)\n",
    "    \n",
    "    flatten = Flatten(name='decoder')\n",
    "    decoder_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, decoder_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Deep Residual Networks for Single Image Super-Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1707.02921\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-3.png\">\n",
    "\n",
    "Recently, residual networks exhibit excellent performance in computer vision problems from the lowlevel to high-level tasks. Although Ledig et al. successfully applied the ResNet architecture to the super-resolution problem with SRResNet, we further improve the performance by employing better ResNet structure.\n",
    "\n",
    "We remove the batch normalization layers from our network as Nah et al.[19] presented in their image deblurring work. Since batch normalization layers normalize the features, they get rid of range flexibility from networks by normalizing the features, it is better to remove them. We experimentally show that this simple modification increases the performance substantially as detailed in\n",
    "\n",
    "Furthermore, GPU memory usage is also sufficiently reduced since the batch normalization layers consume the same amount of memory as the preceding convolutional layers. Our baseline model without batch normalization layer saves approximately 40% of memory usage during training, compared to SRResNet. Consequently, we can build up a larger model that has better performance than conventional ResNet structure under limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(x,x_g,filters):\n",
    "    \n",
    "    current_layer = x ; generator = x_g\n",
    "\n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "\n",
    "    # conv 1\n",
    "    c1 = Conv2D(filters,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "    \n",
    "    # activation 1\n",
    "    a1 = Activation(ACTIVATION)\n",
    "    current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "\n",
    "    # conv 2\n",
    "    c2 = Conv2D(filters,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "    \n",
    "    # residual attention\n",
    "    current_layer, generator = residual_attention(current_layer,generator)\n",
    "\n",
    "    # residual scaling\n",
    "#     scale = Lambda(lambda x: x * R_SCALING)\n",
    "#     current_layer = scale(current_layer) ; generator = scale(generator)\n",
    "        \n",
    "    # merge shortcut\n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "\n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Attention Module for Single Image Super-Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1811.12043\n",
    "\n",
    "Attention mechanisms are a design trend of deep neural networks that stands out in various computer vision tasks. Recently, some works have attempted to apply attention mechanisms to single image super-resolution (SR) tasks. However, they apply the mechanisms to SR in the same or similar ways used for high-level computer vision problems without much consideration of the different nature between SR and other problems. In this paper, we propose a new attention method, which is composed of new channel-wise and spatial attention mechanisms optimized for SR and a new fused attention to combine them. Based on this, we propose a new residual attention module (RAM) and a SR network using RAM (SRRAM). We provide in-depth experimental analysis of different attention mechanisms in SR. It is shown that the proposed method can construct both deep and lightweight SR networks showing improved performance in comparison to existing state-of-the-art methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_attention(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "    \n",
    "    # global variance pooling\n",
    "    gvp = GlobalVariancePooling2D()\n",
    "    channel_attention = gvp(shortcut); channel_attention_g = gvp(shortcut_g)\n",
    "    \n",
    "    # reshape\n",
    "    reshape = Reshape((1,1,R_FILTERS))\n",
    "    channel_attention = reshape(channel_attention); channel_attention_g = reshape(channel_attention_g);\n",
    "    \n",
    "    # squeeze\n",
    "    squeeze = Conv2D(int(R_FILTERS/R_REDUCTION),1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    channel_attention = squeeze(channel_attention); channel_attention_g = squeeze(channel_attention_g);\n",
    "    \n",
    "    # excitation\n",
    "    a1 = Activation(ACTIVATION)\n",
    "    channel_attention = a1(channel_attention); channel_attention_g = a1(channel_attention_g)\n",
    "    \n",
    "    # scaling\n",
    "    c2 = Conv2D(R_FILTERS,1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    channel_attention = c2(channel_attention) ; channel_attention_g = c2(channel_attention_g)\n",
    "        \n",
    "    #spatial attention\n",
    "    dc = DepthwiseConv2D(3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    spatial_attention = dc(shortcut); spatial_attention_g = dc(shortcut_g)\n",
    "    \n",
    "    # fuse channel and spatial attention\n",
    "    add = Add()\n",
    "    current_layer = add([channel_attention,spatial_attention]); generator = add([channel_attention_g,spatial_attention_g])\n",
    "    \n",
    "    # sigmoid activation\n",
    "    s = Activation(\"sigmoid\")\n",
    "    current_layer = s(current_layer) ; generator = s(generator)\n",
    "    \n",
    "    # merge fused attention with shortcut\n",
    "    m = Multiply()\n",
    "    current_layer = m([current_layer,shortcut]) ; generator = m([generator, shortcut_g])\n",
    "    \n",
    "    return current_layer, generator\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(x,x_g,layers,filters,name='refiner'):\n",
    "    c1 = Conv2D(CHANNELS,1,padding='SAME',kernel_initializer=INITIALIZER_TANH)\n",
    "    current_layer = c1(x) ; generator = c1(x_g)\n",
    "    \n",
    "    shortcut = current_layer; shortcut_g = generator\n",
    "    \n",
    "    c2 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(x) ; generator = c2(x_g)\n",
    "\n",
    "    for i in range(layers):\n",
    "        current_layer, generator = residual(current_layer, generator, filters)\n",
    "    \n",
    "    c3 = Conv2D(CHANNELS,1,padding='SAME',kernel_initializer=INITIALIZER_TANH)\n",
    "    current_layer = c3(current_layer); generator = c3(generator)\n",
    "    \n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "    \n",
    "    activation = Activation('tanh',name=name+'_dssim')\n",
    "    current_layer = activation(current_layer) ; generator = activation(generator)\n",
    "    \n",
    "    flatten = Flatten(name=name)\n",
    "    refiner_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, refiner_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1707.02921\n",
    "\n",
    "<img width=500 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-18+at+8.54.43+AM.png\">\n",
    "\n",
    "Recently, the powerful capability of deep neural networks has led to dramatic improvements in SR. Since Dong et al. [4, 5] first proposed a deep learning-based SR method, various CNN architectures have been studied for SR. Kim et al. [11, 12] first introduced the residual network for training much deeper network architectures and achieved superior performance. In particular, they showed that skip connection and recursive convolution alleviate the burden of carrying identity information in the super-resolution network. Similarly to [20], Mao et al. [16] tackled the general image restoration problem with encoder-decoder networks and symmetric skip connections. In [16], they argue that those nested skip connections provide fast and improved convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1609.05158\n",
    "\n",
    "<img width=\"75%\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-23+at+10.26.45+AM.png\">\n",
    "\n",
    "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. \n",
    "\n",
    "In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. \n",
    "\n",
    "We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(x,x_g,filters,name='upsampler'):\n",
    "    \n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # convolution\n",
    "    c1 = Conv2D(filters*SCALE_FACTOR*SCALE_FACTOR,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "    \n",
    "    # activation\n",
    "    a1 = Activation(ACTIVATION)\n",
    "    current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "    \n",
    "    # sub-pixel upscaling\n",
    "    upscale = SubPixelUpscaling(scale_factor=SCALE_FACTOR)\n",
    "    current_layer = upscale(current_layer); generator = upscale(generator)\n",
    "    \n",
    "    # In practice, it is useful to have a second convolution layer after the \n",
    "    # SubPixelUpscaling layer to speed up the learning process.\n",
    "    c2 = Conv2D(filters*SCALE_FACTOR*SCALE_FACTOR,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "    \n",
    "    # activation\n",
    "    a2 = Activation(ACTIVATION)\n",
    "    current_layer = a2(current_layer) ; generator = a2(generator)\n",
    "    \n",
    "    # convolution\n",
    "    c3 = Conv2D(CHANNELS,1,padding='SAME',kernel_initializer=INITIALIZER_TANH)\n",
    "    current_layer = c3(current_layer); generator = c3(generator)\n",
    "    \n",
    "    # activation\n",
    "    a3 = Activation('tanh',name=name+\"_dssim\")\n",
    "    current_layer = a3(current_layer) ; generator = a3(generator)\n",
    "    \n",
    "    # flatten\n",
    "    flatten = Flatten(name=name)\n",
    "    upscale_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, upscale_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1511.07289\n",
    "\n",
    "<img width=\"300\" style=\"float:left;\" src=\"https://blogs.mathworks.com/deep-learning/files/2017/12/defining_elu_layer_01.png\">\n",
    "\n",
    "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. \n",
    "\n",
    "In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. \n",
    "\n",
    "Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default activation\n",
    "ACTIVATION  = 'elu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1502.01852\n",
    "\n",
    "<img style=\"float:left;\" width=\"300\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-24+at+12.24.47+PM.png\">\n",
    "Rectifier networks are easier to train compared with traditional sigmoid-like activation networks. But a bad initialization can still hamper the learning of a highly non-linear system. In this subsection, we propose a robust initialization method that removes an obstacle of training extremely deep rectifier networks.\n",
    "\n",
    "It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where  fan_in is the number of input units in the weight tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters in Action! Part II — Weight Initializers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
    "\n",
    "<img width=\"300\" style=\"float:left;\" src=\"https://cdn-images-1.medium.com/max/1600/1*WLUL_bcjsNK9sXNw6nC-cg.png\">\n",
    "\n",
    "If you dug a little bit deeper, you’ve likely also found out that one should use Xavier / Glorot initialization if the activation function is a Tanh, and that He initialization is the recommended one if the activation function is a ReLU.\n",
    "\n",
    "In summary, for a ReLU activated network, the He initialization scheme using an Uniform distribution is a pretty good choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializers\n",
    "INITIALIZER = 'he_uniform'\n",
    "INITIALIZER_TANH = 'glorot_uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Generate Images with Perceptual Similarity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1511.06409\n",
    "\n",
    "<img width=\"200\" style=\"float:left;\" src=\"https://s3.amazonaws.com/neurokinetikz/download-4.png\">\n",
    "\n",
    "In this paper, we explore loss functions that, unlike MSE, MAE, and likelihoods, are grounded in human perceptual judgments. We show that these perceptual losses lead to representations are superior to other methods, both with respect to reconstructing given images, and generating novel ones. This superiority is demonstrated both in quantitative studies and human judgements ... We (also) demonstrate that perceptual losses yield a convincing win when applied to a state-of-the-art architecture for single image super-resolution.\n",
    "\n",
    "As observed in the deterministic case, MS-SSIM is better at capturing fine details than either MSE or MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gifit(epoch=None):\n",
    "    if (epoch % GIF_STEPS == 0):\n",
    "        print('saving gif ...')\n",
    "        z,y,yc,i,ic,s,sc = AUTOENCODER.predict_on_batch(SAMPLES)\n",
    "        img = np.clip(127.5*(s+1).reshape((-1, SCALE_FACTOR*SIZE, SCALE_FACTOR*SIZE, CHANNELS)), 0, 255)\n",
    "        RECONS.append(utils.montage(img).astype(np.uint8))\n",
    "        \n",
    "def saveit(epoch=None):\n",
    "    if ((epoch > 0) and (epoch % MODEL_STEPS == 0)):\n",
    "        print('saving model ...')\n",
    "        AUTOENCODER.save(MODEL_NAME+'-autoencoder-model.h5')\n",
    "        ENCODER.save(MODEL_NAME+'-encoder-model.h5')\n",
    "        GENERATOR.save(MODEL_NAME+'-generator-model.h5')\n",
    "        SUPER.save(MODEL_NAME+'-super-model.h5')\n",
    "        print('done')\n",
    "       \n",
    "        \n",
    "# callbacks\n",
    "giffer = LambdaCallback(on_epoch_end=lambda epoch, logs: gifit(epoch))\n",
    "saver = LambdaCallback(on_epoch_end=lambda epoch, logs: saveit(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS      = 12501\n",
    "BATCH_SIZE  = 4\n",
    "\n",
    "MODEL_STEPS = 50\n",
    "GIF_STEPS   = 10\n",
    "\n",
    "SAMPLES =  np.random.permutation(FLAT)[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SIZE == 256):\n",
    "    FILTERS = [64,80,96,128,96,64]\n",
    "    \n",
    "elif (SIZE == 128):\n",
    "    FILTERS = [64,96,128,96,64]\n",
    "    \n",
    "elif (SIZE == 64):\n",
    "    FILTERS = [64,96,128,64]\n",
    "    \n",
    "elif (SIZE == 32):\n",
    "    FILTERS = [64,128,64]\n",
    "\n",
    "# Residuals\n",
    "R_LAYERS  = 16\n",
    "R_FILTERS = 32\n",
    "R_SCALING = 0.01\n",
    "\n",
    "# Latent dimension size\n",
    "LATENT_DIM = 512\n",
    "\n",
    "# Residual channel attention\n",
    "# R_GROUPS = 1\n",
    "# R_BLOCKS = 16\n",
    "R_REDUCTION = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "gvp:  (?, 32)\n",
      "reshape:  (?, 1, 1, 32)\n",
      "squeeze:  (?, 1, 1, 16)\n",
      "a1:  (?, 1, 1, 16)\n",
      "c2:  (?, 1, 1, 32)\n",
      "ca:  (?, 1, 1, 32)\n",
      "sa:  (?, 32, 32, 32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_55 (InputLayer)           (None, 3072)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_123 (Reshape)           (None, 32, 32, 3)    0           input_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 32, 32, 64)   1792        reshape_123[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_325 (GroupN (None, 32, 32, 64)   128         conv2d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_506 (Activation)     (None, 32, 32, 64)   0           group_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 32, 32, 64)   36928       activation_506[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_326 (GroupN (None, 32, 32, 64)   128         conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_507 (Activation)     (None, 32, 32, 64)   0           group_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_82 (MaxPooling2D) (None, 16, 16, 64)   0           activation_507[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 16, 16, 128)  73856       max_pooling2d_82[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_327 (GroupN (None, 16, 16, 128)  256         conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_508 (Activation)     (None, 16, 16, 128)  0           group_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 16, 16, 128)  147584      activation_508[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_328 (GroupN (None, 16, 16, 128)  256         conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_509 (Activation)     (None, 16, 16, 128)  0           group_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_83 (MaxPooling2D) (None, 8, 8, 128)    0           activation_509[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 8, 8, 64)     73792       max_pooling2d_83[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_329 (GroupN (None, 8, 8, 64)     128         conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_510 (Activation)     (None, 8, 8, 64)     0           group_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 8, 8, 64)     36928       activation_510[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_330 (GroupN (None, 8, 8, 64)     128         conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_511 (Activation)     (None, 8, 8, 64)     0           group_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_84 (MaxPooling2D) (None, 4, 4, 64)     0           activation_511[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 1024)         0           max_pooling2d_84[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Dense)                 (None, 512)          524800      flatten_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "generator (Dense)               (None, 1024)         525312      encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_124 (Reshape)           (None, 4, 4, 64)     0           generator[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_82 (UpSampling2D) (None, 8, 8, 64)     0           reshape_124[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 8, 8, 64)     36928       up_sampling2d_82[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_331 (GroupN (None, 8, 8, 64)     128         conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_512 (Activation)     (None, 8, 8, 64)     0           group_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 8, 8, 64)     36928       activation_512[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_332 (GroupN (None, 8, 8, 64)     128         conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_513 (Activation)     (None, 8, 8, 64)     0           group_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_83 (UpSampling2D) (None, 16, 16, 64)   0           activation_513[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 16, 16, 128)  73856       up_sampling2d_83[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_333 (GroupN (None, 16, 16, 128)  256         conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_514 (Activation)     (None, 16, 16, 128)  0           group_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 16, 16, 128)  147584      activation_514[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_334 (GroupN (None, 16, 16, 128)  256         conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_515 (Activation)     (None, 16, 16, 128)  0           group_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_84 (UpSampling2D) (None, 32, 32, 128)  0           activation_515[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 32, 32, 64)   73792       up_sampling2d_84[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_335 (GroupN (None, 32, 32, 64)   128         conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_516 (Activation)     (None, 32, 32, 64)   0           group_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 32, 32, 64)   36928       activation_516[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_336 (GroupN (None, 32, 32, 64)   128         conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_517 (Activation)     (None, 32, 32, 64)   0           group_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 32, 32, 3)    195         activation_517[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dssim (Activation)      (None, 32, 32, 3)    0           conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 32, 32, 32)   896         decoder_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 32, 32, 32)   9248        conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_518 (Activation)     (None, 32, 32, 32)   0           conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 32, 32, 32)   9248        activation_518[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_74 (G (None, 32)           0           conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_125 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_74[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 1, 1, 16)     528         reshape_125[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_519 (Activation)     (None, 1, 1, 16)     0           conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_696 (Conv2D)             (None, 1, 1, 32)     544         activation_519[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_56 (DepthwiseC (None, 32, 32, 32)   320         conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_106 (Add)                   (None, 32, 32, 32)   0           conv2d_696[0][0]                 \n",
      "                                                                 depthwise_conv2d_56[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_520 (Activation)     (None, 32, 32, 32)   0           add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_49 (Multiply)          (None, 32, 32, 32)   0           activation_520[0][0]             \n",
      "                                                                 conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_107 (Add)                   (None, 32, 32, 32)   0           multiply_49[0][0]                \n",
      "                                                                 conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_697 (Conv2D)             (None, 32, 32, 32)   9248        add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_521 (Activation)     (None, 32, 32, 32)   0           conv2d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_698 (Conv2D)             (None, 32, 32, 32)   9248        activation_521[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_75 (G (None, 32)           0           conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_126 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_75[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_699 (Conv2D)             (None, 1, 1, 16)     528         reshape_126[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_522 (Activation)     (None, 1, 1, 16)     0           conv2d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_700 (Conv2D)             (None, 1, 1, 32)     544         activation_522[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_57 (DepthwiseC (None, 32, 32, 32)   320         conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_108 (Add)                   (None, 32, 32, 32)   0           conv2d_700[0][0]                 \n",
      "                                                                 depthwise_conv2d_57[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_523 (Activation)     (None, 32, 32, 32)   0           add_108[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_50 (Multiply)          (None, 32, 32, 32)   0           activation_523[0][0]             \n",
      "                                                                 conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_109 (Add)                   (None, 32, 32, 32)   0           multiply_50[0][0]                \n",
      "                                                                 add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_701 (Conv2D)             (None, 32, 32, 32)   9248        add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_524 (Activation)     (None, 32, 32, 32)   0           conv2d_701[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_702 (Conv2D)             (None, 32, 32, 32)   9248        activation_524[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_76 (G (None, 32)           0           conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_127 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_76[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_703 (Conv2D)             (None, 1, 1, 16)     528         reshape_127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_525 (Activation)     (None, 1, 1, 16)     0           conv2d_703[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_704 (Conv2D)             (None, 1, 1, 32)     544         activation_525[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_58 (DepthwiseC (None, 32, 32, 32)   320         conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_110 (Add)                   (None, 32, 32, 32)   0           conv2d_704[0][0]                 \n",
      "                                                                 depthwise_conv2d_58[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_526 (Activation)     (None, 32, 32, 32)   0           add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_51 (Multiply)          (None, 32, 32, 32)   0           activation_526[0][0]             \n",
      "                                                                 conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_111 (Add)                   (None, 32, 32, 32)   0           multiply_51[0][0]                \n",
      "                                                                 add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_705 (Conv2D)             (None, 32, 32, 32)   9248        add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_527 (Activation)     (None, 32, 32, 32)   0           conv2d_705[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_706 (Conv2D)             (None, 32, 32, 32)   9248        activation_527[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_77 (G (None, 32)           0           conv2d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_128 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_77[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_707 (Conv2D)             (None, 1, 1, 16)     528         reshape_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_528 (Activation)     (None, 1, 1, 16)     0           conv2d_707[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_708 (Conv2D)             (None, 1, 1, 32)     544         activation_528[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_59 (DepthwiseC (None, 32, 32, 32)   320         conv2d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_112 (Add)                   (None, 32, 32, 32)   0           conv2d_708[0][0]                 \n",
      "                                                                 depthwise_conv2d_59[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_529 (Activation)     (None, 32, 32, 32)   0           add_112[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_52 (Multiply)          (None, 32, 32, 32)   0           activation_529[0][0]             \n",
      "                                                                 conv2d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_113 (Add)                   (None, 32, 32, 32)   0           multiply_52[0][0]                \n",
      "                                                                 add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_709 (Conv2D)             (None, 32, 32, 32)   9248        add_113[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_530 (Activation)     (None, 32, 32, 32)   0           conv2d_709[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_710 (Conv2D)             (None, 32, 32, 32)   9248        activation_530[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_78 (G (None, 32)           0           conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_129 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_78[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_711 (Conv2D)             (None, 1, 1, 16)     528         reshape_129[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_531 (Activation)     (None, 1, 1, 16)     0           conv2d_711[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_712 (Conv2D)             (None, 1, 1, 32)     544         activation_531[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_60 (DepthwiseC (None, 32, 32, 32)   320         conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 32, 32, 32)   0           conv2d_712[0][0]                 \n",
      "                                                                 depthwise_conv2d_60[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_532 (Activation)     (None, 32, 32, 32)   0           add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_53 (Multiply)          (None, 32, 32, 32)   0           activation_532[0][0]             \n",
      "                                                                 conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 32, 32, 32)   0           multiply_53[0][0]                \n",
      "                                                                 add_113[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_713 (Conv2D)             (None, 32, 32, 32)   9248        add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_533 (Activation)     (None, 32, 32, 32)   0           conv2d_713[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_714 (Conv2D)             (None, 32, 32, 32)   9248        activation_533[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_79 (G (None, 32)           0           conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_130 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_79[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_715 (Conv2D)             (None, 1, 1, 16)     528         reshape_130[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_534 (Activation)     (None, 1, 1, 16)     0           conv2d_715[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_716 (Conv2D)             (None, 1, 1, 32)     544         activation_534[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_61 (DepthwiseC (None, 32, 32, 32)   320         conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 32, 32, 32)   0           conv2d_716[0][0]                 \n",
      "                                                                 depthwise_conv2d_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_535 (Activation)     (None, 32, 32, 32)   0           add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_54 (Multiply)          (None, 32, 32, 32)   0           activation_535[0][0]             \n",
      "                                                                 conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 32, 32, 32)   0           multiply_54[0][0]                \n",
      "                                                                 add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_717 (Conv2D)             (None, 32, 32, 32)   9248        add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_536 (Activation)     (None, 32, 32, 32)   0           conv2d_717[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_718 (Conv2D)             (None, 32, 32, 32)   9248        activation_536[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_80 (G (None, 32)           0           conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_131 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_80[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_719 (Conv2D)             (None, 1, 1, 16)     528         reshape_131[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_537 (Activation)     (None, 1, 1, 16)     0           conv2d_719[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_720 (Conv2D)             (None, 1, 1, 32)     544         activation_537[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_62 (DepthwiseC (None, 32, 32, 32)   320         conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 32, 32, 32)   0           conv2d_720[0][0]                 \n",
      "                                                                 depthwise_conv2d_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 32, 32, 32)   0           add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_55 (Multiply)          (None, 32, 32, 32)   0           activation_538[0][0]             \n",
      "                                                                 conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_119 (Add)                   (None, 32, 32, 32)   0           multiply_55[0][0]                \n",
      "                                                                 add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_721 (Conv2D)             (None, 32, 32, 32)   9248        add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 32, 32, 32)   0           conv2d_721[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_722 (Conv2D)             (None, 32, 32, 32)   9248        activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_81 (G (None, 32)           0           conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_132 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_81[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_723 (Conv2D)             (None, 1, 1, 16)     528         reshape_132[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 1, 1, 16)     0           conv2d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_724 (Conv2D)             (None, 1, 1, 32)     544         activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_63 (DepthwiseC (None, 32, 32, 32)   320         conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_120 (Add)                   (None, 32, 32, 32)   0           conv2d_724[0][0]                 \n",
      "                                                                 depthwise_conv2d_63[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 32, 32, 32)   0           add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_56 (Multiply)          (None, 32, 32, 32)   0           activation_541[0][0]             \n",
      "                                                                 conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_121 (Add)                   (None, 32, 32, 32)   0           multiply_56[0][0]                \n",
      "                                                                 add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_725 (Conv2D)             (None, 32, 32, 32)   9248        add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 32, 32, 32)   0           conv2d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_726 (Conv2D)             (None, 32, 32, 32)   9248        activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_82 (G (None, 32)           0           conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_133 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_82[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_727 (Conv2D)             (None, 1, 1, 16)     528         reshape_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 1, 1, 16)     0           conv2d_727[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_728 (Conv2D)             (None, 1, 1, 32)     544         activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_64 (DepthwiseC (None, 32, 32, 32)   320         conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_122 (Add)                   (None, 32, 32, 32)   0           conv2d_728[0][0]                 \n",
      "                                                                 depthwise_conv2d_64[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 32, 32, 32)   0           add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_57 (Multiply)          (None, 32, 32, 32)   0           activation_544[0][0]             \n",
      "                                                                 conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_123 (Add)                   (None, 32, 32, 32)   0           multiply_57[0][0]                \n",
      "                                                                 add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_729 (Conv2D)             (None, 32, 32, 32)   9248        add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 32, 32, 32)   0           conv2d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_730 (Conv2D)             (None, 32, 32, 32)   9248        activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_83 (G (None, 32)           0           conv2d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_134 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_83[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_731 (Conv2D)             (None, 1, 1, 16)     528         reshape_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 1, 1, 16)     0           conv2d_731[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_732 (Conv2D)             (None, 1, 1, 32)     544         activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_65 (DepthwiseC (None, 32, 32, 32)   320         conv2d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_124 (Add)                   (None, 32, 32, 32)   0           conv2d_732[0][0]                 \n",
      "                                                                 depthwise_conv2d_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 32, 32, 32)   0           add_124[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_58 (Multiply)          (None, 32, 32, 32)   0           activation_547[0][0]             \n",
      "                                                                 conv2d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_125 (Add)                   (None, 32, 32, 32)   0           multiply_58[0][0]                \n",
      "                                                                 add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_733 (Conv2D)             (None, 32, 32, 32)   9248        add_125[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 32, 32, 32)   0           conv2d_733[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_734 (Conv2D)             (None, 32, 32, 32)   9248        activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_84 (G (None, 32)           0           conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_135 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_84[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_735 (Conv2D)             (None, 1, 1, 16)     528         reshape_135[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 1, 1, 16)     0           conv2d_735[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_736 (Conv2D)             (None, 1, 1, 32)     544         activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_66 (DepthwiseC (None, 32, 32, 32)   320         conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_126 (Add)                   (None, 32, 32, 32)   0           conv2d_736[0][0]                 \n",
      "                                                                 depthwise_conv2d_66[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 32, 32, 32)   0           add_126[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_59 (Multiply)          (None, 32, 32, 32)   0           activation_550[0][0]             \n",
      "                                                                 conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_127 (Add)                   (None, 32, 32, 32)   0           multiply_59[0][0]                \n",
      "                                                                 add_125[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_737 (Conv2D)             (None, 32, 32, 32)   9248        add_127[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 32, 32, 32)   0           conv2d_737[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_738 (Conv2D)             (None, 32, 32, 32)   9248        activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_85 (G (None, 32)           0           conv2d_738[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_136 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_85[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_739 (Conv2D)             (None, 1, 1, 16)     528         reshape_136[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 1, 1, 16)     0           conv2d_739[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_740 (Conv2D)             (None, 1, 1, 32)     544         activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_67 (DepthwiseC (None, 32, 32, 32)   320         conv2d_738[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_128 (Add)                   (None, 32, 32, 32)   0           conv2d_740[0][0]                 \n",
      "                                                                 depthwise_conv2d_67[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 32, 32, 32)   0           add_128[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_60 (Multiply)          (None, 32, 32, 32)   0           activation_553[0][0]             \n",
      "                                                                 conv2d_738[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_129 (Add)                   (None, 32, 32, 32)   0           multiply_60[0][0]                \n",
      "                                                                 add_127[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_741 (Conv2D)             (None, 32, 32, 32)   9248        add_129[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 32, 32, 32)   0           conv2d_741[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_742 (Conv2D)             (None, 32, 32, 32)   9248        activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_86 (G (None, 32)           0           conv2d_742[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_137 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_86[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_743 (Conv2D)             (None, 1, 1, 16)     528         reshape_137[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 1, 1, 16)     0           conv2d_743[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_744 (Conv2D)             (None, 1, 1, 32)     544         activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_68 (DepthwiseC (None, 32, 32, 32)   320         conv2d_742[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_130 (Add)                   (None, 32, 32, 32)   0           conv2d_744[0][0]                 \n",
      "                                                                 depthwise_conv2d_68[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 32, 32, 32)   0           add_130[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_61 (Multiply)          (None, 32, 32, 32)   0           activation_556[0][0]             \n",
      "                                                                 conv2d_742[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_131 (Add)                   (None, 32, 32, 32)   0           multiply_61[0][0]                \n",
      "                                                                 add_129[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_745 (Conv2D)             (None, 32, 32, 32)   9248        add_131[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 32, 32, 32)   0           conv2d_745[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_746 (Conv2D)             (None, 32, 32, 32)   9248        activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_87 (G (None, 32)           0           conv2d_746[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_138 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_87[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_747 (Conv2D)             (None, 1, 1, 16)     528         reshape_138[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 1, 1, 16)     0           conv2d_747[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_748 (Conv2D)             (None, 1, 1, 32)     544         activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_69 (DepthwiseC (None, 32, 32, 32)   320         conv2d_746[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_132 (Add)                   (None, 32, 32, 32)   0           conv2d_748[0][0]                 \n",
      "                                                                 depthwise_conv2d_69[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 32, 32, 32)   0           add_132[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_62 (Multiply)          (None, 32, 32, 32)   0           activation_559[0][0]             \n",
      "                                                                 conv2d_746[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_133 (Add)                   (None, 32, 32, 32)   0           multiply_62[0][0]                \n",
      "                                                                 add_131[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_749 (Conv2D)             (None, 32, 32, 32)   9248        add_133[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 32, 32, 32)   0           conv2d_749[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_750 (Conv2D)             (None, 32, 32, 32)   9248        activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_88 (G (None, 32)           0           conv2d_750[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_139 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_88[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_751 (Conv2D)             (None, 1, 1, 16)     528         reshape_139[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 1, 1, 16)     0           conv2d_751[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_752 (Conv2D)             (None, 1, 1, 32)     544         activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_70 (DepthwiseC (None, 32, 32, 32)   320         conv2d_750[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_134 (Add)                   (None, 32, 32, 32)   0           conv2d_752[0][0]                 \n",
      "                                                                 depthwise_conv2d_70[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 32, 32, 32)   0           add_134[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_63 (Multiply)          (None, 32, 32, 32)   0           activation_562[0][0]             \n",
      "                                                                 conv2d_750[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_135 (Add)                   (None, 32, 32, 32)   0           multiply_63[0][0]                \n",
      "                                                                 add_133[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_753 (Conv2D)             (None, 32, 32, 32)   9248        add_135[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 32, 32, 32)   0           conv2d_753[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_754 (Conv2D)             (None, 32, 32, 32)   9248        activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_89 (G (None, 32)           0           conv2d_754[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_140 (Reshape)           (None, 1, 1, 32)     0           global_variance_pooling2d_89[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_755 (Conv2D)             (None, 1, 1, 16)     528         reshape_140[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 1, 1, 16)     0           conv2d_755[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_756 (Conv2D)             (None, 1, 1, 32)     544         activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_71 (DepthwiseC (None, 32, 32, 32)   320         conv2d_754[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_136 (Add)                   (None, 32, 32, 32)   0           conv2d_756[0][0]                 \n",
      "                                                                 depthwise_conv2d_71[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 32, 32, 32)   0           add_136[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_64 (Multiply)          (None, 32, 32, 32)   0           activation_565[0][0]             \n",
      "                                                                 conv2d_754[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_137 (Add)                   (None, 32, 32, 32)   0           multiply_64[0][0]                \n",
      "                                                                 add_135[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_757 (Conv2D)             (None, 32, 32, 3)    99          add_137[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 32, 32, 3)    12          decoder_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_138 (Add)                   (None, 32, 32, 3)    0           conv2d_757[0][0]                 \n",
      "                                                                 conv2d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "refiner_dssim (Activation)      (None, 32, 32, 3)    0           add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_758 (Conv2D)             (None, 32, 32, 128)  3584        refiner_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 32, 32, 128)  0           conv2d_758[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sub_pixel_upscaling_3 (SubPixel (None, 64, 64, 32)   0           activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_759 (Conv2D)             (None, 64, 64, 128)  36992       sub_pixel_upscaling_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 64, 64, 128)  0           conv2d_759[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_760 (Conv2D)             (None, 64, 64, 3)    387         activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "super_dssim (Activation)        (None, 64, 64, 3)    0           conv2d_760[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Flatten)               (None, 3072)         0           decoder_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "refiner (Flatten)               (None, 3072)         0           refiner_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "super (Flatten)                 (None, 12288)        0           super_dssim[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,189,429\n",
      "Trainable params: 2,189,429\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "X = Input(shape=(FEATURES,))\n",
    "\n",
    "# latent\n",
    "Z, shape = encode(X)\n",
    "\n",
    "# generator input\n",
    "Z_G = Input(shape=(LATENT_DIM,))\n",
    "\n",
    "# coarse reconstruction\n",
    "Y, YG, Y_F = decode(Z,Z_G,shape)\n",
    "\n",
    "# fine reconstruction\n",
    "IMG, IMG_G, IMG_F = refine(Y,YG,R_LAYERS,R_FILTERS)\n",
    "\n",
    "# super image resolution\n",
    "IMG_S, IMG_G_S, IMG_S_F = upsample(IMG,IMG_G,R_FILTERS,name='super')\n",
    "   \n",
    "# define autoencoder\n",
    "AUTOENCODER = Model(inputs=[X], outputs=[Z,Y,Y_F,IMG,IMG_F,IMG_S,IMG_S_F])\n",
    "\n",
    "# define encoder\n",
    "ENCODER = Model(inputs=[X], outputs=[Z])\n",
    "\n",
    "# define generator\n",
    "GENERATOR = Model(inputs=[Z_G], outputs=[IMG_G])\n",
    "\n",
    "# define super imager\n",
    "SUPER = Model(inputs=[Z_G], outputs=[IMG_G_S])\n",
    "\n",
    "# define optimizer\n",
    "ADAM = optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=True)\n",
    "\n",
    "# define losses\n",
    "losses = {'encoder':vae_loss,\n",
    "          'decoder':'mse',\n",
    "          'refiner':'mae',\n",
    "          'super':'mae',\n",
    "          'decoder_dssim':DSSIMObjective(),\n",
    "          'refiner_dssim':DSSIMObjective(),\n",
    "          'super_dssim':DSSIMObjective()}\n",
    "\n",
    "# compile models\n",
    "AUTOENCODER.compile(optimizer=ADAM,loss=losses)\n",
    "ENCODER.compile(optimizer=ADAM,loss='mse')\n",
    "GENERATOR.compile(optimizer=ADAM,loss='mae')\n",
    "SUPER.compile(optimizer=ADAM,loss='mae')\n",
    "\n",
    "# print summary\n",
    "AUTOENCODER.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12501\n",
      "176/184 [===========================>..] - ETA: 2s - loss: 4.0518 - encoder_loss: 0.0182 - decoder_dssim_loss: 0.5034 - decoder_loss: 1.0159 - refiner_dssim_loss: 0.4995 - refiner_loss: 0.9709 - super_dssim_loss: 0.4968 - super_loss: 0.5471"
     ]
    }
   ],
   "source": [
    "RECONS = []\n",
    "\n",
    "# fit model\n",
    "AUTOENCODER.fit(x=FLAT,y=[FLAT,IMGS,FLAT,IMGS,FLAT,IMGS_2X,FLAT_2X],batch_size=BATCH_SIZE,epochs=EPOCHS,callbacks=[giffer,saver])\n",
    "\n",
    "# save training gif\n",
    "gif.build_gif(RECONS, saveto=MODEL_NAME+'-final'+ \"-\"+str(time.time())+'.gif')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading encoder ...', MODEL_NAME)\n",
    "ENCODER = load_model(MODEL_NAME+'-encoder-model.h5')\n",
    "\n",
    "print('loading generator ...')\n",
    "GENERATOR = load_model(MODEL_NAME+'-generator-model.h5',custom_objects={'R_SCALING':R_SCALING})\n",
    "\n",
    "print('loading super ...')\n",
    "SUPER = load_model(MODEL_NAME+'-super-model.h5',custom_objects={'R_SCALING':R_SCALING})\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=600 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-13+at+12.17.10+PM.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(index=0):\n",
    "    \n",
    "    # input\n",
    "    x = np.reshape(FLAT[index],(-1,FEATURES))\n",
    "    z = ENCODER.predict_on_batch(x)\n",
    "    \n",
    "    # output\n",
    "    img = np.reshape(GENERATOR.predict_on_batch(z),(-1,FEATURES))\n",
    "    img_s = np.reshape(SUPER.predict_on_batch(z),(-1,FEATURES*SCALE_FACTOR*SCALE_FACTOR))\n",
    "    \n",
    "    # reference\n",
    "    ref = IMGS[index]/2 + .5\n",
    "    ref_s = IMGS_2X[index]/2 + .5\n",
    "    \n",
    "    # denormalize\n",
    "    img = np.reshape(img/2 + .5,(SIZE,SIZE,CHANNELS))\n",
    "    img_s= np.reshape(img_s/2 + .5,(SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS))\n",
    "    \n",
    "    # print scores\n",
    "    print(\"PSNR: %.3f %.3f <> MS-SSIM: %.3f %.3f\" % ((utils.psnr(ref,img)),\n",
    "                                                     (utils.psnr(ref_s,img_s)),\n",
    "                                           (utils.MultiScaleSSIM(np.reshape(ref,(1,SIZE,SIZE,CHANNELS)),\n",
    "                                                                 np.reshape(img,(1,SIZE,SIZE,CHANNELS)),\n",
    "                                                                 max_val=1.)),\n",
    "                                            (utils.MultiScaleSSIM(np.reshape(ref_s,(1,SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS)),\n",
    "                                                                 np.reshape(img_s,(1,SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS)),\n",
    "                                                                 max_val=1.))\n",
    "                                               ))\n",
    "    \n",
    "    # show images\n",
    "    utils.showImagesHorizontally(images=[ref,img])\n",
    "    utils.showImagesHorizontally(images=[ref_s,img_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct(random.randint(0,TOTAL_BATCH-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent  Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.youtube.com/watch?v=grEi3uRlSb4\n",
    "<table><tr>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-109.gif\"></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-100.gif\"></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-080.gif\"></td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_latents(n_imgs=3,steps=30):\n",
    "    rimgs = np.random.permutation(FLAT)[:n_imgs]\n",
    "    rimgs = np.append(rimgs, [rimgs[0]],axis=0)\n",
    "    latent_animation(rimgs,steps,filename=str(time.time()))\n",
    "\n",
    "def latent_animation(imgs=None,steps=None,filename=\"latent-animation\"):\n",
    "    \n",
    "    # get latent encodings for images\n",
    "    print('getting latent vectors ...')\n",
    "    latents = []\n",
    "    for index,img in enumerate(imgs):\n",
    "        img = np.reshape(img,(-1,FEATURES))\n",
    "        latent = ENCODER.predict_on_batch(img)\n",
    "        latents.append(latent)\n",
    "\n",
    "    # calculate latent path\n",
    "    print('calculating latent path ...')\n",
    "    latent_path = []\n",
    "    for i in range(len(latents)-1):\n",
    "        # get latent vectors\n",
    "        l1 = latents[i] ; l2 = latents[i+1]\n",
    "\n",
    "        # calculate latent distance\n",
    "        image_distance = l2 - l1\n",
    "\n",
    "        # create the latent path\n",
    "        for j in range(steps):\n",
    "            latent_path.append(l1 + j*image_distance/steps)\n",
    "        latent_path.append(l2) \n",
    "    \n",
    "    # reconstruct images along the path\n",
    "    print('reconstructing latent paths... ')\n",
    "    latent_path = np.reshape(latent_path,(-1,LATENT_DIM))\n",
    "#     recons = GENERATOR.predict_on_batch(latent_path)\n",
    "\n",
    "#     # de-normalize and clip the output\n",
    "#     final = np.clip((127.5*(recons+1)).reshape((-1,SIZE,SIZE,CHANNELS)),0,255)\n",
    "\n",
    "#     # build the gif\n",
    "#     gif.build_gif([utils.montage([r]).astype(np.uint8) for r in final], saveto=filename+\".gif\",dpi=32)\n",
    "    \n",
    "    recons = SUPER.predict_on_batch(latent_path)\n",
    "    final = np.clip((127.5*(recons+1)).reshape((-1,SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS)),0,255)\n",
    "    gif.build_gif([utils.montage([r]).astype(np.uint8) for r in final], saveto=filename+\"-2x.gif\",dpi=32)\n",
    "\n",
    "    # done\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LATENT_DIM=512\n",
    "for i in range(100):\n",
    "    random_latents(2,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs =  np.random.permutation(FLAT)\n",
    "t = str(time.time())\n",
    "for i in range(TOTAL_BATCH):\n",
    "    print(i)\n",
    "    latent_animation([imgs[i],imgs[i+1]],50,filename=t+'-'+ ('%03d' % i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model & Continue Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('load model ...')\n",
    "AUTOENCODER = load_model(MODEL_NAME+'-autoencoder-model.h5',\n",
    "                         custom_objects={'vae_loss': vae_loss, \n",
    "                                         'R_SCALING':0.1, \n",
    "                                         'DSSIMObjective':DSSIMObjective()})\n",
    "\n",
    "# define encoder\n",
    "ENCODER = Model(inputs=[AUTOENCODER.input], outputs=[AUTOENCODER.get_layer(\"encoder\").output])\n",
    "\n",
    "# define generator\n",
    "Z = Input(shape=(LATENT_DIM,))\n",
    "GENERATOR = Model(inputs=[Z], outputs=[AUTOENCODER.get_layer(\"generator\")(Z)])\n",
    "\n",
    "ENCODER.compile(optimizer=ADAM,loss='mse')\n",
    "GENERATOR.compile(optimizer=ADAM,loss='mse')\n",
    "\n",
    "print('resume training ...')\n",
    "AUTOENCODER.fit(x=FLAT,y=[FLAT,IMGS,FLAT,IMGS,FLAT],batch_size=BATCH_SIZE,epochs=EPOCHS,callbacks=[giffer,saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Channel Attention Network (RCAN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=600 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-18+at+6.08.27+PM.png\">\n",
    "\n",
    "Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_group(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "    \n",
    "    for i in range(R_BLOCKS):\n",
    "        # attention block\n",
    "        current_layer, generator = attention_block(current_layer, generator)\n",
    "        \n",
    "        # final convolution\n",
    "        if(i+1 == R_BLOCKS):\n",
    "            conv = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "            current_layer = conv(current_layer) ; generator = conv(generator)\n",
    "    \n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "    \n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Channel Attention Block (RCAB) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-18+at+6.06.32+PM.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "    \n",
    "     # conv 1\n",
    "    c1 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "    \n",
    "    # activation 1\n",
    "    a1 = Activation(ACTIVATION)\n",
    "    current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "\n",
    "    # conv 2\n",
    "    c2 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "    \n",
    "    scale = Lambda(lambda x: x * R_SCALING)\n",
    "    current_layer = scale(current_layer) ; generator = scale(generator)\n",
    "    \n",
    "    # channel attention\n",
    "    current_layer, generator = channel_attention(current_layer,generator)\n",
    "    \n",
    "    # merge shortcut\n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "    \n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "    \n",
    "    # global average pooling\n",
    "    gp = GlobalAveragePooling2D()\n",
    "    current_layer = gp(current_layer); generator = gp(generator);\n",
    "    \n",
    "    reshape = Reshape((1,1,R_FILTERS))\n",
    "    current_layer = reshape(current_layer); generator = reshape(generator);\n",
    "        \n",
    "     # conv 1\n",
    "    c1 = Conv2D(int(R_FILTERS/R_REDUCTION),1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "\n",
    "    # activation 1\n",
    "    a = Activation(ACTIVATION)\n",
    "    current_layer = a(current_layer) ; generator = a(generator)\n",
    "    \n",
    "    # conv 2\n",
    "    c2 = Conv2D(R_FILTERS,1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "    \n",
    "    # residual scaling\n",
    "    scale = Lambda(lambda x: x * R_SCALING)\n",
    "    current_layer = scale(current_layer) ; generator = scale(generator)\n",
    "    \n",
    "    # sigmoid activation\n",
    "    s = Activation(\"sigmoid\")\n",
    "    current_layer = s(current_layer) ; generator = s(generator)\n",
    "    \n",
    "    # merge shortcut\n",
    "    m = Multiply()\n",
    "    current_layer = m([current_layer,shortcut]) ; generator = m([generator,shortcut_g])\n",
    "    \n",
    "    return current_layer, generator\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception: Deep Learning with Depthwise Separable Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1610.02357\n",
    "\n",
    "<img width=\"600\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-24+at+11.19.12+AM.png\">\n",
    "\n",
    "https://stackoverflow.com/questions/52248655/depthwise-separable-convolution/52550451\n",
    "\n",
    "The intuition behind doing this is to decouple the spatial information (width and height) and the depthwise information (channels). While regular convolutional layers will merge feature maps over the number of input channels, depthwise separable convolutions will perform another 1x1 convolution before adding them up.\n",
    "\n",
    "Using a depthwise separable convolutional layer as a drop-in replacement for a regular one will greatly reduce the number of weights in the model. It will also very likely hurt the accuracy due to the much smaller number of weights. However, if you change the width and depth of your architecture to increase the weights again, you may reach the same accuracy of the original model with less parameters. At the same time a depthwise separable model with the same number of weights might achieve a higher accuracy compared the original model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
