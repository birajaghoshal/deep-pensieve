{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Pensieve™\n",
    "A Residual Multi-Stage Maximum Mean Discrepancy Variational Resize-Convolution Auto-Encoder with Group Normalization and Structural Similarity Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540551741.4923084-final.gif'></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540552110.470284-final.gif'></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-080.gif\"></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540552122.395882-final.gif'></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540551578.5925505-final.gif'></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1705.07202\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-1.png\">\n",
    "\n",
    "Variational auto-encoder (VAE) is a powerful unsupervised learning framework for image generation. One drawback of VAE is that it generates blurry images due to its Gaussianity assumption and thus L2 loss. To allow the generation of high quality images by VAE, we increase the capacity of decoder network by employing residual blocks and skip connections, which also enable efficient optimization. To overcome the limitation of L2 loss, we propose to generate images in a multi-stage manner from coarse to fine. In the simplest case, the proposed multi-stage VAE divides the decoder into two components in which the second component generates refined images based on the course images generated by the first component. Since the second component is independent of the VAE model, it can employ other loss functions beyond the L2 loss and different model architectures. The proposed framework can be easily generalized to contain more than two components. Experiment results on the MNIST and CelebA datasets demonstrate that the proposed multi-stage VAE can generate sharper images as compared to those from the original VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from libs import utils, gif\n",
    "from libs.group_norm import GroupNormalization\n",
    "\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.layers import Input, Flatten, Reshape, Add, Activation, Lambda\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras_contrib.losses import DSSIMObjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIRECTORY = 'roadtrip'\n",
    "\n",
    "SIZE = 256\n",
    "CHANNELS = 3\n",
    "FEATURES = SIZE*SIZE*CHANNELS\n",
    "\n",
    "MODEL_NAME = DIRECTORY+'-'+str(SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load images\n",
    "imgs, xs, ys = utils.load_images(directory=\"imgs/\"+DIRECTORY,rx=SIZE,ry=SIZE)\n",
    "\n",
    "# normalize pixels\n",
    "IMGS = imgs/127.5 - 1\n",
    "FLAT = np.reshape(IMGS,(-1,FEATURES))\n",
    "SAMPLES =  np.random.permutation(FLAT)[:9]\n",
    "TOTAL_BATCH = IMGS.shape[0]\n",
    "\n",
    "# print shapes\n",
    "print(\"MODEL: \",MODEL_NAME)\n",
    "print(\"IMGS: \",IMGS.shape)\n",
    "print(\"FLAT: \",FLAT.shape)\n",
    "print(\"SAMPLES: \",SAMPLES.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/groupnorm.png\">\n",
    "\n",
    "\n",
    "The mainstream normalization technique for almost all convolutional neural networks today is Batch Normalization (BN), which has been widely adopted in the development of deep learning. Proposed by Google in 2015, BN can not only accelerate a model’s converging speed, but also alleviate problems such as Gradient Dispersion in the deep neural network, making it easier to train models.\n",
    "\n",
    "Dr. Wu and Dr. He however argue in their paper Group Normalization that normalizing with batch size has limitations, as BN cannot ensure the model accuracy rate when the batch size becomes smaller. As a result, researchers today are normalizing with large batches, which is very memory intensive, and are avoiding using limited memory to explore higher-capacity models.\n",
    "\n",
    "Dr. Wu and Dr. He believe their new GN technique is a simple but effective alternative to BN. Specifically, GN divides channels — also referred to as feature maps that look like 3D chunks of data — into groups and normalizes the features within each group. GN only exploits the layer dimensions, and its computation is independent of batch sizes.\n",
    "\n",
    "The paper reports that GN had a 10.6% lower error rate than its BN counterpart for ResNet-50 in ImageNet with a batch size of 2 samples; and matched BN performance while outperforming other normalization techniques with a regular batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    # set current layer\n",
    "    current_layer = Reshape((SIZE,SIZE,CHANNELS))(x)\n",
    "    \n",
    "    # convolution layers\n",
    "    for layer, n_filters in enumerate(FILTERS):\n",
    "\n",
    "        # stacked 3x3 convolutions with group normalization + activation\n",
    "        current_layer = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)(current_layer)\n",
    "        current_layer = GroupNormalization(groups=n_filters,axis=-1)(current_layer)\n",
    "        current_layer = Activation(ACTIVATION)(current_layer)\n",
    "\n",
    "        current_layer = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)(current_layer)\n",
    "        current_layer = GroupNormalization(groups=n_filters,axis=-1)(current_layer)\n",
    "        current_layer = Activation(ACTIVATION)(current_layer)\n",
    "         \n",
    "        # max pooling\n",
    "        current_layer = MaxPooling2D()(current_layer)\n",
    "    \n",
    "    # grab the last shape for reconstruction\n",
    "    shape = current_layer.get_shape().as_list()\n",
    "    \n",
    "    # flatten\n",
    "    flat = Flatten()(current_layer)\n",
    "    \n",
    "    # latent vector\n",
    "    z = Dense(LATENT_DIM,name='encoder')(flat)\n",
    "    \n",
    "    return z, (shape[1],shape[2],shape[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Mean Discrepancy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/kl_latent.gif\" ></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/mmd_latent.gif\" ></td>\n",
    "</tr></table>\n",
    "\n",
    "Maximum mean discrepancy (MMD, (Gretton et al. 2007)) is based on the idea that two distributions are identical if and only if all their moments are the same. Therefore, we can define a divergence by measuring how “different” the moments of two distributions p(z) and q(z) are. MMD can accomplish this efficiently via the kernel embedding trick:\n",
    "\n",
    "A kernel can be intuitively interpreted as a function that measures the “similarity” of two samples. It has a large value when two samples are similar, and small when they are different. For example, the Gaussian kernel considers points that are close in Euclidean space to be “similar”. A rough intuition of MMD, then, is that if two distributions are identical, then the average “similarity” between samples from each distribution, should be identical to the average “similarity” between mixed samples from both distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_kernel(x, y):\n",
    "    x_size = tf.shape(x)[0]\n",
    "    y_size = tf.shape(y)[0]\n",
    "    dim = tf.shape(x)[1]\n",
    "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vae_loss(y_true,y_pred):\n",
    "    epsilon = tf.random_normal(tf.stack([BATCH_SIZE, LATENT_DIM]))\n",
    "    latent_loss = compute_mmd(epsilon, y_pred)\n",
    "    return latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution and Checkerboard Artifacts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "When we have neural networks generate images, we often have them build them up from low resolution, high-level descriptions. This allows the network to describe the rough image and then fill in the details.\n",
    "\n",
    "In order to do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. Roughly, deconvolution layers allow the model to use every point in the small image to “paint” a square in the larger one.\n",
    "\n",
    "Unfortunately, deconvolution can easily have “uneven overlap,” putting more of the metaphorical paint in some places than others. In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top). While the network could, in principle, carefully learn weights to avoid this  — as we’ll discuss in more detail later — in practice neural networks struggle to avoid it completely.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-2.png\">\n",
    "\n",
    "To avoid these artifacts, we’d like an alternative to regular deconvolution (“transposed convolution”). Unlike deconvolution, this approach to upsampling shouldn’t have artifacts as its default behavior. Ideally, it would go further, and be biased against such artifacts.\n",
    "\n",
    "One approach is to separate out upsampling to a higher resolution from convolution to compute features. For example, you might resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer. This seems like a natural approach, and roughly similar methods have worked well in image super-resolution.\n",
    "\n",
    "Our experience has been that nearest-neighbor resize followed by a convolution works very well, in a wide variety of contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(z,z_g,shape=None):\n",
    "    \n",
    "    # reverse the encoder\n",
    "    filters = FILTERS[::-1]\n",
    "\n",
    "    # inflate\n",
    "    inflated = shape[0]*shape[1]*shape[2]\n",
    "    inflate = Dense(inflated,name='generator')\n",
    "    current_layer = inflate(z) ; generator = inflate(z_g)\n",
    "    \n",
    "    # reshape\n",
    "    reshape = Reshape(shape)\n",
    "    current_layer = reshape(current_layer) ; generator = reshape(generator)\n",
    "    \n",
    "    # build layers\n",
    "    for layer, n_filters in enumerate(filters):\n",
    "        \n",
    "        # upsample\n",
    "        u = UpSampling2D()\n",
    "        current_layer = u(current_layer) ; generator = u(generator)\n",
    "\n",
    "        # stacked 3x3 convolutions with group normalization + activation\n",
    "        c1 = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "        b1 = GroupNormalization(groups=n_filters,axis=-1)\n",
    "        a1 = Activation(ACTIVATION)\n",
    "\n",
    "        current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "        current_layer = b1(current_layer) ; generator = b1(generator)\n",
    "        current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "\n",
    "        c2 = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "        b2 = GroupNormalization(groups=n_filters,axis=-1)\n",
    "        a2 = Activation(ACTIVATION)\n",
    "\n",
    "        current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "        current_layer = b2(current_layer) ; generator = b2(generator)\n",
    "        current_layer = a2(current_layer) ; generator = a2(generator)\n",
    "    \n",
    "    # output convolution + activation\n",
    "    conv = Conv2D(CHANNELS,1,padding='SAME')\n",
    "    activation = Activation('tanh',name='decoder_dssim')\n",
    "    \n",
    "    current_layer = conv(current_layer)       ; generator = conv(generator)\n",
    "    current_layer = activation(current_layer) ; generator = activation(generator)\n",
    "    \n",
    "    flatten = Flatten(name='decoder')\n",
    "    decoder_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, decoder_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Deep Residual Networks for Single Image Super-Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1707.02921\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-3.png\">\n",
    "\n",
    "Recently, residual networks exhibit excellent performance in computer vision problems from the lowlevel to high-level tasks. Although Ledig et al. successfully applied the ResNet architecture to the super-resolution problem with SRResNet, we further improve the performance by employing better ResNet structure.\n",
    "\n",
    "We remove the batch normalization layers from our network as Nah et al.[19] presented in their image deblurring work. Since batch normalization layers normalize the features, they get rid of range flexibility from networks by normalizing the features, it is better to remove them. We experimentally show that this simple modification increases the performance substantially as detailed in\n",
    "\n",
    "Furthermore, GPU memory usage is also sufficiently reduced since the batch normalization layers consume the same amount of memory as the preceding convolutional layers. Our baseline model without batch normalization layer saves approximately 40% of memory usage during training, compared to SRResNet. Consequently, we can build up a larger model that has better performance than conventional ResNet structure under limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual(x,x_g):\n",
    "    \n",
    "    current_layer = x ; generator = x_g\n",
    "\n",
    "    # shortcuts\n",
    "    shortcut = current_layer \n",
    "    shortcut_g = generator\n",
    "\n",
    "    # conv 1\n",
    "    c1 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "    \n",
    "    # activation 1\n",
    "    a1 = Activation(ACTIVATION)\n",
    "    current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "\n",
    "    # conv 2\n",
    "    c2 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "\n",
    "    # residual scaling\n",
    "    scale = Lambda(lambda x: x * R_SCALING)\n",
    "    current_layer = scale(current_layer) ; generator = scale(generator)\n",
    "\n",
    "    # fix shortcut shape if mismatch\n",
    "    if(shortcut.shape[-1] != current_layer.shape[-1]):\n",
    "        s = Conv2D(R_FILTERS,1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "        shortcut = s(current_layer) ; shortcut_g = s(generator)\n",
    "        \n",
    "    # merge shortcut\n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "\n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refine(x,x_g):\n",
    "    \n",
    "    # reshape\n",
    "    reshape = Reshape((SIZE,SIZE,CHANNELS))\n",
    "    current_layer = reshape(x) ; generator = reshape(x_g)\n",
    "\n",
    "    # residual layers\n",
    "    for layer in range(R_LAYERS):\n",
    "    \n",
    "        # residual block\n",
    "        current_layer, generator = residual(current_layer,generator)\n",
    "    \n",
    "    # output convolution + activation\n",
    "    conv = Conv2D(CHANNELS,1,padding='SAME')\n",
    "    activation = Activation('tanh',name='refiner_dssim')\n",
    "    \n",
    "    current_layer = conv(current_layer)       ; generator = conv(generator)\n",
    "    current_layer = activation(current_layer) ; generator = activation(generator)\n",
    "    \n",
    "    flatten = Flatten(name='refiner')\n",
    "    refiner_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, refiner_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Generate Images with Perceptual Similarity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1511.06409\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-4.png\">\n",
    "\n",
    "In this paper, we explore loss functions that, unlike MSE, MAE, and likelihoods, are grounded in human perceptual judgments. We show that these perceptual losses lead to representations are superior to other methods, both with respect to reconstructing given images (Figure 1), and generating novel ones. This superiority is demonstrated both in quantitative studies and human judgements. Beyond achieving perceptually superior synthesized images, we also show that that our perceptually-optimized representations are better suited for image classification. Finally, we demonstrate that perceptual losses yield a convincing win when applied to a state-of-the-art architecture for single image super-resolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RECONS = []\n",
    "\n",
    "def gifit(epoch=None):\n",
    "    if (epoch % GIF_STEPS == 0):\n",
    "        print('saving gif ...')\n",
    "        z,y,yc,i,ic = AUTOENCODER.predict_on_batch(SAMPLES)\n",
    "        img = np.clip(127.5*(i+1).reshape((-1, SIZE, SIZE, CHANNELS)), 0, 255)\n",
    "        RECONS.append(utils.montage(img).astype(np.uint8))\n",
    "        \n",
    "def saveit(epoch=None):\n",
    "    if (epoch == 0):\n",
    "        print('saving model ...')\n",
    "        AUTOENCODER.save(MODEL_NAME+'-autoencoder-model.h5')\n",
    "        \n",
    "        print('saving encoder ...') \n",
    "        ENCODER.save(MODEL_NAME+'-encoder-model.h5')\n",
    "        \n",
    "        print('saving generator ...')\n",
    "        GENERATOR.save(MODEL_NAME+'-generator-model.h5')\n",
    "    \n",
    "    elif (epoch % MODEL_STEPS == 0):\n",
    "        print('saving autoencoder ...')\n",
    "        AUTOENCODER.save(MODEL_NAME+'-autoencoder-model.h5')\n",
    "        \n",
    "        print('saving encoder ...')\n",
    "        ENCODER.save(MODEL_NAME+'-encoder-model.h5')\n",
    "        \n",
    "        print('saving generator ...')\n",
    "        GENERATOR.save(MODEL_NAME+'-generator-model.h5')\n",
    "        \n",
    "# callbacks\n",
    "giffer = LambdaCallback(on_epoch_end=lambda epoch, logs: gifit(epoch))\n",
    "saver = LambdaCallback(on_epoch_end=lambda epoch, logs: saveit(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS      = 12501\n",
    "BATCH_SIZE  = 4\n",
    "\n",
    "MODEL_STEPS = 10\n",
    "GIF_STEPS   = 100\n",
    "\n",
    "SAMPLES =  np.random.permutation(FLAT)[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 512px\n",
    "# FILTERS = [64,80,96,112,96,80,64]\n",
    "\n",
    "# 256px\n",
    "FILTERS = [64,96,128,160,128,64]\n",
    "\n",
    "# 128px\n",
    "#FILTERS = [64,128,196,128,64]\n",
    "\n",
    "# 64px\n",
    "#FILTERS = [32,48,64,48]\n",
    "\n",
    "# 32px\n",
    "# FILTERS = [32,64,32]\n",
    "\n",
    "# Default initializer and activation\n",
    "INITIALIZER = 'he_normal'\n",
    "ACTIVATION  = 'elu'\n",
    "\n",
    "# Residuals\n",
    "R_LAYERS  = 24\n",
    "R_FILTERS = 64\n",
    "R_SCALING = 0.1\n",
    "\n",
    "# Latent dimension size\n",
    "LATENT_DIM = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "X = Input(shape=(FEATURES,),name='input')\n",
    "\n",
    "# latent\n",
    "Z, shape = encode(X)\n",
    "\n",
    "# generator input\n",
    "Z_G = Input(shape=(LATENT_DIM,))\n",
    "\n",
    "# coarse reconstruction\n",
    "Y, YG, Y_F = decode(Z,Z_G,shape)\n",
    "\n",
    "# fine reconstruction\n",
    "IMG, IMG_G, IMG_F = refine(Y,YG)\n",
    "    \n",
    "# define autoencoder\n",
    "AUTOENCODER = Model(inputs=[X], outputs=[Z,Y,Y_F,IMG,IMG_F])\n",
    "\n",
    "# define encoder\n",
    "ENCODER = Model(inputs=[X], outputs=[Z])\n",
    "\n",
    "# define generator\n",
    "GENERATOR = Model(inputs=[Z_G], outputs=[IMG_G])\n",
    "\n",
    "# define optimizer\n",
    "ADAM = optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=True)\n",
    "\n",
    "# define losses\n",
    "losses = {'encoder':vae_loss,\n",
    "          'decoder':'mse',\n",
    "          'refiner':'mae',\n",
    "          'decoder_dssim':DSSIMObjective(),\n",
    "          'refiner_dssim':DSSIMObjective() }\n",
    "\n",
    "# compile models\n",
    "AUTOENCODER.compile(optimizer=ADAM,loss=losses)\n",
    "ENCODER.compile(optimizer=ADAM,loss='mse')\n",
    "GENERATOR.compile(optimizer=ADAM,loss='mse')\n",
    "\n",
    "# print summary\n",
    "AUTOENCODER.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "AUTOENCODER.fit(x=FLAT,y=[FLAT,IMGS,FLAT,IMGS,FLAT],batch_size=BATCH_SIZE,epochs=EPOCHS,callbacks=[giffer,saver])\n",
    "\n",
    "# save training gif\n",
    "gif.build_gif(RECONS, saveto=MODEL_NAME+'-final'+ \"-\"+str(time.time())+'.gif')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('loading encoder ...', MODEL_NAME)\n",
    "ENCODER = load_model(MODEL_NAME+'-encoder-model.h5')\n",
    "\n",
    "print('loading generator ...')\n",
    "GENERATOR = load_model(MODEL_NAME+'-generator-model.h5',custom_objects={'R_SCALING':0.1})\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=600 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-13+at+12.17.10+PM.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct(index=0):\n",
    "    x = np.reshape(FLAT[index],(-1,FEATURES))\n",
    "    z = ENCODER.predict_on_batch(x)\n",
    "    img = np.reshape(GENERATOR.predict_on_batch(z),(-1,FEATURES))\n",
    "    \n",
    "    ref = IMGS[index]/2 + .5\n",
    "    img = np.reshape(img/2 + .5,(SIZE,SIZE,CHANNELS))\n",
    "    \n",
    "    print(\"PSNR: %.3f <> MS-SSIM: %.3f\" % ((utils.psnr(ref,img)),\n",
    "                                           (utils.MultiScaleSSIM(np.reshape(ref,(1,SIZE,SIZE,CHANNELS)),\n",
    "                                                                 np.reshape(img,(1,SIZE,SIZE,CHANNELS)),\n",
    "                                                                 max_val=1.))))\n",
    "    \n",
    "    utils.showImagesHorizontally(images=[ref,img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruct(random.randint(0,TOTAL_BATCH-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent  Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.youtube.com/watch?v=grEi3uRlSb4\n",
    "<table><tr>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-109.gif\"></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-100.gif\"></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-080.gif\"></td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_latents(n_imgs=3,steps=30):\n",
    "    rimgs = np.random.permutation(FLAT)[:n_imgs]\n",
    "    rimgs = np.append(rimgs, [rimgs[0]],axis=0)\n",
    "    latent_animation(rimgs,steps,filename=str(time.time()))\n",
    "\n",
    "def latent_animation(imgs=None,steps=None,filename=\"latent-animation\"):\n",
    "    \n",
    "    # get latent encodings for images\n",
    "    print('getting latent vectors ...')\n",
    "    latents = []\n",
    "    for index,img in enumerate(imgs):\n",
    "        img = np.reshape(img,(-1,FEATURES))\n",
    "        latent = ENCODER.predict_on_batch(img)\n",
    "        latents.append(latent)\n",
    "\n",
    "    # calculate latent path\n",
    "    print('calculating latent path ...')\n",
    "    latent_path = []\n",
    "    for i in range(len(latents)-1):\n",
    "        # get latent vectors\n",
    "        l1 = latents[i] ; l2 = latents[i+1]\n",
    "\n",
    "        # calculate latent distance\n",
    "        image_distance = l2 - l1\n",
    "\n",
    "        # create the latent path\n",
    "        for j in range(steps):\n",
    "            latent_path.append(l1 + j*image_distance/steps)\n",
    "        latent_path.append(l2) \n",
    "    \n",
    "    # reconstruct images along the path\n",
    "    print('reconstructing latent paths... ')\n",
    "    latent_path = np.reshape(latent_path,(-1,LATENT_DIM))\n",
    "    recons = GENERATOR.predict_on_batch(latent_path)\n",
    "\n",
    "    # de-normalize and clip the output\n",
    "    final = np.clip((127.5*(recons+1)).reshape((-1,SIZE,SIZE,CHANNELS)),0,255)\n",
    "\n",
    "    # build the gif\n",
    "    gif.build_gif([utils.montage([r]).astype(np.uint8) for r in final], saveto=filename+\".gif\",dpi=256)\n",
    "\n",
    "    # done\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    random_latents(2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgs =  np.random.permutation(FLAT)\n",
    "t = str(time.time())\n",
    "for i in range(TOTAL_BATCH):\n",
    "    print(i)\n",
    "    latent_animation([imgs[i],imgs[i+1]],200,filename=t+'-'+ ('%03d' % i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model & Continue Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('load model ...')\n",
    "AUTOENCODER = load_model(MODEL_NAME+'-autoencoder-model.h5',\n",
    "                         custom_objects={'vae_loss': vae_loss, \n",
    "                                         'R_SCALING':0.1, \n",
    "                                         'DSSIMObjective':DSSIMObjective()})\n",
    "\n",
    "# define encoder\n",
    "ENCODER = Model(inputs=[AUTOENCODER.input], outputs=[AUTOENCODER.get_layer(\"encoder\").output])\n",
    "\n",
    "# define generator\n",
    "Z = Input(shape=(LATENT_DIM,))\n",
    "GENERATOR = Model(inputs=[Z], outputs=[AUTOENCODER.get_layer(\"generator\")(Z)])\n",
    "\n",
    "ENCODER.compile(optimizer=ADAM,loss='mse')\n",
    "GENERATOR.compile(optimizer=ADAM,loss='mse')\n",
    "\n",
    "print('resume training ...')\n",
    "AUTOENCODER.fit(x=FLAT,y=[FLAT,IMGS,FLAT,IMGS,FLAT],batch_size=BATCH_SIZE,epochs=EPOCHS,callbacks=[giffer,saver])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
